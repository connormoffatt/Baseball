##########
# Ec/ACM/CS 112, Spring 2018
# Code. HW 5 - Part 2
# Connor Moffatt
##########
# clear workspace
rm(list=ls())
# set random seed
set.seed(2018)
# libraries
library(rjags)
library(coda)
#########
# SET-UP DATA
#########
# data
nGroups = 6
y1 = c(83, 92, 92, 46)
y2 = c(117, 109, 114, 104)
y3 = c(101, 93, 92, 86)
y4 = c(105, 119, 116, 102)
y5 = c(79, 97, 103, 79)
y6 = c(57, 92, 104, 77)
# data to be passed to JAGS
# NOTE:
# ++ Must be in list format
# ++ For each variable, left denotes the name of the variable, right denotes the current values
# ++ Must match names in model file
y = c(y1, y2, y3, y4, y5, y6)
nTotal = length(y)
group = c(rep(1, length(y1)), rep(2, length(y2)), rep(3, length(y3)),
rep(4, length(y4)), rep(5, length(y5)), rep(6, length(y6)))
dataList = list(nTotal = nTotal,
nGroups = nGroups,
y = y,
group = group)
#####
# INITIALIZE CHAINS & SAMPLE
#####
# init parameters for sampler
# NOTE:
# ++ Must be in list format
# ++ Left hand side denotes name of each variable, and must match names in model file
# ++ Can initialize multiple chains using lists of lists
# ++ Can also initialize using a function that returns a randomly generated list
initList = function() {
theta = c(sample(y1,1), sample(y2,1), sample(y3,1), sample(y4,1),
sample(y5,1), sample(y6,1))
mu = rnorm(1, 0, 100)
sigma = runif(1, 0, 10)
tau = runif(1, 0, 10)
return(list(theta = theta, mu = mu, sigma = sigma, tau = tau))
}
# compile and initialize model
jagsModel = jags.model(file = "C:/Users/conno/Documents/Caltech/Junior 3rd Term/Acm 112/Moffatt Connor PS5/HW5_Part2_Try2.txt",
data = dataList,
inits = initList(),
n.chains = 1,
n.adapt = 10000)
postSamples = coda.samples(model = jagsModel,
variable.names = c("theta", "sigma", "mu", "tau"),
n.iter = 100000)
postSamples = do.call(rbind, postSamples)
summary(postSamples, quantiles=c(0.01, 0.05, 0.5, 0.95,0.99))
count = 0
for (i in 1:100000) {
theta1 = as.double(postSamples[i,4])
theta5 = as.double(postSamples[i,8])
if (theta5 < theta1) {
count = count + 1
}
}
prob_theta5_less_theta1 = count / 100000
postSamples1 = mcmc(postSamples)
postSamples = do.call(rbind, postSamples)
postSamples1 = mcmc(postSamples)
summary(postSamples1, quantiles=c(0.01, 0.05, 0.5, 0.95,0.99))
count = 0
for (i in 1:100000) {
theta1 = as.double(postSamples[i,4])
theta5 = as.double(postSamples[i,8])
if (theta5 < theta1) {
count = count + 1
}
}
prob_theta5_less_theta1 = count / 100000
prob_theta5_less_theta1
##########
# Ec/ACM/CS 112, Spring 2018
# Code. HW 5 - Part 3
# Connor Moffatt
##########
# clear workspace
rm(list=ls())
# set random seed
set.seed(2018)
# libraries
library(rjags)
library(coda)
#########
# SET-UP DATA
#########
# data
nGroups = 6
y1 = c(83, 92, 92, 46)
y2 = c(117, 109, 114, 104)
y3 = c(101, 93, 92, 86)
y4 = c(105, 119, 116, 102)
y5 = c(79, 97, 103, 79)
y6 = c(57, 92, 104, 77)
# data to be passed to JAGS
# NOTE:
# ++ Must be in list format
# ++ For each variable, left denotes the name of the variable, right denotes the current values
# ++ Must match names in model file
y = c(y1, y2, y3, y4, y5, y6)
nTotal = length(y)
group = c(rep(1, length(y1)), rep(2, length(y2)), rep(3, length(y3)),
rep(4, length(y4)), rep(5, length(y5)), rep(6, length(y6)))
dataList = list(nTotal = nTotal,
nGroups = nGroups,
y = y,
group = group)
#####
# INITIALIZE CHAINS & SAMPLE
#####
# init parameters for sampler
# NOTE:
# ++ Must be in list format
# ++ Left hand side denotes name of each variable, and must match names in model file
# ++ Can initialize multiple chains using lists of lists
# ++ Can also initialize using a function that returns a randomly generated list
initList = function() {
theta = c(sample(y1,1), sample(y2,1), sample(y3,1), sample(y4,1),
sample(y5,1), sample(y6,1))
mu = rnorm(1, 0, 100)
sigma = runif(1, 0, 10)
tau = runif(1, 0, 10)
return(list(theta = theta, mu = mu, sigma = sigma, tau = tau))
}
# compile and initialize model
jagsModel = jags.model(file = "C:/Users/conno/Documents/Caltech/Junior 3rd Term/Acm 112/Moffatt Connor PS5/HW5_Part2_Try2.txt",
data = dataList,
inits = initList(),
n.chains = 1,
n.adapt = 10000)
postSamples = coda.samples(model = jagsModel,
variable.names = c("theta", "sigma", "mu", "tau"),
n.iter = 100000)
postSamples = do.call(rbind, postSamples)
# Calculate Predictive Distribution for Hierarchical
result_mat1 = array(0, c(100000,6))
# Plot
for (i in 1:6) {
for (j in 1:100000) {
result_mat1[j, i] = rnorm(1, postSamples[j, i+3], postSamples[j, 2])
}
}
result_mat1
#####
# INITIALIZE CHAINS & SAMPLE
#####
# init parameters for sampler
# NOTE:
# ++ Must be in list format
# ++ Left hand side denotes name of each variable, and must match names in model file
# ++ Can initialize multiple chains using lists of lists
# ++ Can also initialize using a function that returns a randomly generated list
initList = function() {
theta = c(sample(y1,1), sample(y2,1), sample(y3,1), sample(y4,1),
sample(y5,1), sample(y6,1))
sigma = runif(6, 0, 10)
return(list(theta = theta, sigma = sigma))
}
# compile and initialize model
jagsModel2 = jags.model(file = "C:/Users/conno/Documents/Caltech/Junior 3rd Term/Acm 112/Moffatt Connor PS5/HW5_Part3.txt",
data = dataList,
inits = initList(),
n.chains = 1,
n.adapt = 10000)
postSamples2 = coda.samples(model = jagsModel2,
variable.names = c("theta", "sigma"),
n.iter = 100000)
postSamples2 = do.call(rbind, postSamples2)
# Calculate Predictive Distribution for Separate Models
result_mat2 = array(0, c(100000,6))
# Plot
for (i in 1:6) {
for (j in 1:100000) {
result_mat2[j, i] = rnorm(1, postSamples2[j, i + 6], postSamples2[j, i])
}
}
result_mat2
# Calculate MSEs
act_meas = c(67, 87, 67, 116, 92, 100)
MSE1 = rep(0,6)
MSE2 = rep(0,6)
for (i in 1:6) {
MSE1[i] = mean((result_mat1[,i] - act_meas[i])^2)
MSE2[i] = mean((result_mat2[,i] - act_meas[i])^2)
}
avg_MSE1 = mean(MSE1)
avg_MSE2 = mean(MSE2)
std_MSE1 = sd(MSE1)
std_MSE2 = sd(MSE2)
##########
# Ec/ACM/CS 112, Spring 2018
# Code. HW 5 - Part 2
# Connor Moffatt
##########
# clear workspace
rm(list=ls())
# set random seed
set.seed(2018)
# libraries
library(rjags)
library(coda)
#########
# SET-UP DATA
#########
# data
nGroups = 6
y1 = c(83, 92, 92, 46)
y2 = c(117, 109, 114, 104)
y3 = c(101, 93, 92, 86)
y4 = c(105, 119, 116, 102)
y5 = c(79, 97, 103, 79)
y6 = c(57, 92, 104, 77)
# data to be passed to JAGS
# NOTE:
# ++ Must be in list format
# ++ For each variable, left denotes the name of the variable, right denotes the current values
# ++ Must match names in model file
y = c(y1, y2, y3, y4, y5, y6)
nTotal = length(y)
group = c(rep(1, length(y1)), rep(2, length(y2)), rep(3, length(y3)),
rep(4, length(y4)), rep(5, length(y5)), rep(6, length(y6)))
dataList = list(nTotal = nTotal,
nGroups = nGroups,
y = y,
group = group)
#####
# INITIALIZE CHAINS & SAMPLE
#####
# init parameters for sampler
# NOTE:
# ++ Must be in list format
# ++ Left hand side denotes name of each variable, and must match names in model file
# ++ Can initialize multiple chains using lists of lists
# ++ Can also initialize using a function that returns a randomly generated list
initList = function() {
theta = c(sample(y1,1), sample(y2,1), sample(y3,1), sample(y4,1),
sample(y5,1), sample(y6,1))
mu = rnorm(1, 0, 100)
sigma = runif(1, 0, 10)
tau = runif(1, 0, 10)
return(list(theta = theta, mu = mu, sigma = sigma, tau = tau))
}
# compile and initialize model
jagsModel = jags.model(file = "C:/Users/conno/Documents/Caltech/Junior 3rd Term/Acm 112/Moffatt Connor PS5/HW5_Part2_Try2.txt",
data = dataList,
inits = initList(),
n.chains = 1,
n.adapt = 10000)
postSamples = coda.samples(model = jagsModel,
variable.names = c("theta", "sigma", "mu", "tau"),
n.iter = 100000)
postSamples1 = mcmc(postSamples)
postSamples = do.call(rbind, postSamples)
summary(postSamples1, quantiles=c(0.01, 0.05, 0.5, 0.95,0.99))
count = 0
for (i in 1:100000) {
theta1 = as.double(postSamples[i,4])
theta5 = as.double(postSamples[i,8])
if (theta5 < theta1) {
count = count + 1
}
}
prob_theta5_less_theta1 = count / 100000
postSamples
summary(postSamples1, quantiles=c(0.01, 0.05, 0.5, 0.95,0.99))
summary(postSamples)
1:10 %>%
map(rnorm, n = 10) %>%
map_dbl(mean)
library(purrr)
install.packages(purrr)
install.packages("purrr")
library.packages("readr")
libarary(readr)
install.packages("readr")
install.packages("lme4")
# install.packages("lme4"
lmm.data <- read.table("http://www.unt.edu/rss/class/Jon/R_SC/Module9/lmm.data.txt",
header=True, sep=",", na.strings="NA", dec=".",
strip.white=TRUE)
lmm.data <- read.table("http://www.unt.edu/rss/class/Jon/R_SC/Module9/lmm.data.txt",
header=TRUE, sep=",", na.strings="NA", dec=".",
strip.white=TRUE)
summary(lmm.data)
head(lmm.data)
chapter_path = "C:/Users/conno/Documents/GitHub/Baseball/Modeling"
setwd(chapter_path)
lmm.data <- read.table("lmm.data.txt",
header=TRUE, sep=",", na.strings="NA", dec=".",
strip.white=TRUE)
summary(lmm.data)
head(lmm.data)
# Fit the model using the lmer function
lmm.2 <- lmer(formula = extro ~ open + agree + social + class +
(1|school/class), data = lmm.data, family = gaussian,
REM = TRUE, verbose = FALSE)
# Read in the Example Data
library(lme4)
library(arm) # convenience functions for regression in R
install.packages("arm")
library(arm) # convenience functions for regression in R
# Fit the model using the lmer function
lmm.2 <- lmer(formula = extro ~ open + agree + social + class +
(1|school/class), data = lmm.data, family = gaussian,
REM = TRUE, verbose = FALSE)
# Fit the model using the lmer function
lmm.2 <- lmer(formula = extro ~ open + agree + social + class +
(1|school/class), data = lmm.data)
summary(lmm.2)
# Three values shown for random effects in the form of variances and std. dev.
# If we add the variance components. then we can divide by our nested
# effect variance  to give us the proportion of variance accounted for,
# which indicates whether or not this is meaningful
2.88 + 95.17 + 0.97
2.88 / 99.02
# Only 2.9% of the total variance of the random effects is attributed to the
# nested effect. If all the percentages for each random effect are very small
# then the random effects are not present and the linear mixed model is not
# appropriate. We can see the effect of the school alone is quite substantial
99.17 / 99.02
# Extract estimates of the fixed effects
fixef(lmm.2)
# Extract estimates of random effects
ranef(lmm.2)
# Extract coefficients for the random effects intercept and each group of the
# random effect factor
coef(lmm.2)
# Extracted fitted or predicted values based on model parameters and ata
yhat <- fitted(lmm.2)
summary(yhat)
# Extract residuals and summarize them
residuals <- resid(lmm.2)
summary(residuals)
hist(residuals)
# ICC (Intra Class Correlation) represents a measure of reliability, or
# dependence among individuals. Allows us to assess whether or not the random
# effect is present in the data. First create a null model; which for the
# current example would include just the intercepts and the random effect for
# the highest level variable of the nested structure
lmm.null <- lmer(extro ~ 1 + (1|school), data = lmm.data)
summary(lmm.null)
# Next, add the random effect variance estimates and divide the random effect
# of school's variance estimate by the total variance estiamte
95.87 / (95.87 + 7.14)
# We see that the ICC is 0.9306
# Another way to get ICC is with the multilevel package
aov.1 <- aov(extro ~ school, lmm.data)
summary(aov.1)
library(multilevel)
install.packages("multilevel")
library(multilevel)
ICC1(aov.1)
# We can also get the ICC2, which is a measure of reliability
ICC2(aov.1)
# This high value indicates that school groups can be very reliably differentiated
# in terms of 'extro' scores
detach("package:multilevel")
# To obtain a simulated empirical distribution or posterior distribution
# of estiamtes based on specified lmer model using MCMC methods
mcmc.5000 <- mcmcsamp(lmm.2, saveb=TRUE, n=5000)
?mcmcsamp
# To obtain a simulated empirical distribution or posterior distribution
# of estiamtes based on specified lmer model using MCMC methods
mcmc.5000 <- mcmcsamp(lmm.2, saveb=TRUE, n=5000)
mcmcsamp(lmm.2)
mcmcsamp(lmm.2)
mcmcsamp(lmm.2, n=5000)
library(lme4)
mcmcsamp(lmm.2, n=5000)
# Follow tutorial given at the follwoing website
# https://www.jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r
# https://it.unt.edu/sites/default/files/linearmixedmodels_jds_dec2010.pdf
# Getting Started with Mixed Effect Models in R
# Data Retrived from:
# http://bayes.acs.unt.edu:8083/BayesContent/class/Jon/R_SC/Module9/lmm.data.txt
# Using lme4 package
# install.packages("lme4")
chapter_path = "C:/Users/conno/Documents/GitHub/Baseball/Modeling"
setwd(chapter_path)
# Read in the Data
library(lme4)
library(arm) # convenience functions for regression in R
lmm.data <- read.table("lmm.data.txt",
header=TRUE, sep=",", na.strings="NA", dec=".",
strip.white=TRUE)
# Fit hte non-multilevel models
# Fit a simple OLS regerssion of measures of openness, agreeableness, and
# sociability on extroversion. We use display from arm for abbreviated output
OLSexamp <- lm(extro ~ open + agree + social, daat = lmm.data)
head(lmm.data)
# Follow tutorial given at the follwoing website
# https://www.jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r
# https://it.unt.edu/sites/default/files/linearmixedmodels_jds_dec2010.pdf
# Getting Started with Mixed Effect Models in R
# Data Retrived from:
# http://bayes.acs.unt.edu:8083/BayesContent/class/Jon/R_SC/Module9/lmm.data.txt
# Using lme4 package
# install.packages("lme4")
chapter_path = "C:/Users/conno/Documents/GitHub/Baseball/Modeling"
setwd(chapter_path)
# Read in the Data
library(lme4)
library(arm) # convenience functions for regression in R
lmm.data <- read.table("lmm.data.txt",
header=TRUE, sep=",", na.strings="NA", dec=".",
strip.white=TRUE)
# Fit hte non-multilevel models
# Fit a simple OLS regerssion of measures of openness, agreeableness, and
# sociability on extroversion. We use display from arm for abbreviated output
OLSexamp <- lm(extro ~ open + agree + social, daat = lmm.data)
# Fit hte non-multilevel models
# Fit a simple OLS regerssion of measures of openness, agreeableness, and
# sociability on extroversion. We use display from arm for abbreviated output
OLSexamp <- lm(extro ~ open + agree + social, data = lmm.data)
display(OLSexamp)
# If we want to extract measures such as the AIC, we may prefer to fit a
# generalized linear model with glm which produces a model fit through
# maximum likelihood estimation. Note that the model formula specification is
# the same
MLexamp <- glm(extro ~ open + agree + social, data=lmm.data)
display(MLexamp)
AIC(MLexamp)
# Fit a varying intercept model. Using a grouping variable such as school or
# class
MLexamp.2 <- glm(extro ~ open + agree + social + class, data = lmm.data)
display(MLexamp.2)
AIC(MLexamp.2)
anova(MLexamp, MLexamp.2, test = "F")
# Let us see if school is any better
MLexamp.3 <- glm(extro ~ open + agree + social + school, data=lmm.data)
display(MLexamp.3)
AIC(MLexamp.3)
anova(MLexamp, MLexamp.3, test="F")
# The school effect greatly improves our model fit. However, how do we interpret
# these effects
table(lmm.data$school, lmm.data$class)
# Let's try to model each of these unique cells. To do this we fit a model and
# use the : operator to specify interaction between school and class
MLexamp.4 <- glm(extro ~ open + agree + social + school:class, data = lmm.data)
display(MLexamp.4)
AIC(MLexamp.4)
# This is very useful, but what if we want to understand both the effect of the
# school and the effect of the class, as well as the effect of the schools and
# classes? Unfortunately this is not easily done with standard glm
MLexamp.5 <- glm(extro ~ open = agree + social + school * class - 1,
data=lmm.data)
# This is very useful, but what if we want to understand both the effect of the
# school and the effect of the class, as well as the effect of the schools and
# classes? Unfortunately this is not easily done with standard glm
MLexamp.5 <- glm(extro ~ open = agree + social + school * class - 1, data=lmm.data)
# This is very useful, but what if we want to understand both the effect of the
# school and the effect of the class, as well as the effect of the schools and
# classes? Unfortunately this is not easily done with standard glm
MLexamp.5 <- glm(extro ~ open + agree + social + school * class - 1,
data=lmm.data)
display(MLexamp.5)
AIC(MLexamp.5)
# Exploring Random Slopes
# another alternative is to fit a separate model for each of the school and
# class combinations. If we believe the relationship between our variabels
# may be highly dependent on the school and class combination, we can simply
# fit a series of models and explore the parameter variation among them
require(plyr)
modellist <- dlply(lmm.data, .(school, class), function(x) glm(extro ~ open +
agree + social, data = x))
display(modellist[[1]])
display(modellist[[2]])
# Fit a varying intercept model with lmer
# While all of the above techniques are valid approaches to this problem, they
# are not necessarily the best approach when we are interested explicitly in
# variation among and by groups. This is where a mixed-effect modeling framework
# is useful. Now we use the lmer function witht eh familiar formula interface
# but now group level variables are specified using a special syntax (1|school)
# tells lmer to fit a lienar model witha  varying intercept group using the
# variable school
MLexamp.6 <- lmer(extro ~ open + agree + social + (1| school), data=lmm.data)
display(MLexamp.6)
# We can fit multiple group effects with multiple group effect terms
MLexamp.7 <- lmer(extro ~ open + agree + socail + (1| school) + (1|class),
data=lmm.data)
# We can fit multiple group effects with multiple group effect terms
MLexamp.7 <- lmer(extro ~ open + agree + social + (1| school) + (1|class),
data=lmm.data)
# finally we ca fit nested group effect terms through the following syntax
MLexamp.8 <- lmer(extro ~ open + agree + social + (1| school/class),
data=lmm.data)
display(MLexamp.8)
# Fita  varying slope model with lmer. Instead of fitting unique models by school,
# we can fit a varying slope model. Here we modify our random effect term to
# include variables before the grouping terms: (1 + open|school/class) tells
# R to fit a varying slope and varying intercept model for schools and classes
# nested within schools, and to allow the slope of the open variable to vary
# by school
MLexamp.9 <- lmer(extro ~ open + agree + social + (1 + open | school/class),
data=lmm.data)
display(MLexamp.9)
